ollama {
  localhost = "http://localhost:11434"
  dockerhost = "http://ollama-container:11434"
  model = "llama3.2"
  query-timeout = 300
  iterations = 2
}

maxWords = 100
awsLambdaApiGateway = "https://gbwdh5x7g8.execute-api.us-east-2.amazonaws.com/test/llm-bedrock"
env = "local"           # possible value: {local, docker}

local-output-path = "src/main/resources/"
docker-output-path = "/llm/"
